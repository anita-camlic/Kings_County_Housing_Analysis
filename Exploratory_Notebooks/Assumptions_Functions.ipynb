{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "from matplotlib import pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINEARITY FUNCTION\n",
    "# THE RELATIONSHIP BETWEEN THE TARGET AND PREDICTOR SHOULD BE LINEAR\n",
    "def linearity_test(endog, exog):\n",
    "    predictions = sm.OLS(endog=endog, exog=exog).fit().predict(exog)\n",
    "    residuals = endog - predictions\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(predictions, residuals)\n",
    "    ax.set_xlabel('predicted Y')\n",
    "    ax.set_ylabel('residual values')\n",
    "    plt.suptitle('Residuals Vs. Predictions');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INDEPENDENCE FUNCTION\n",
    "def independence_dest_DW(endog,exog):\n",
    "    ''' POarameters Taken: endog, exog\n",
    "        Returns: the durbin Watson Test Statistic\n",
    "        Details: The Durbin-Watson test statistic is calculated on the null hypothesis that there is no correlation among the errors. \n",
    "        The test statistic has a range of 0 to 4, where 2 indicates no correlation, a score less than 2 indicates a positive correlation, and a score greater than 2 indicates a negative correlation.\n",
    "    '''\n",
    "    predictions = sm.OLS(endog=endog, exog=exog).fit().predict(exog)\n",
    "    residuals = endog - predictions\n",
    "    dw = sm.stats.stattools.durbin_watson\n",
    "    return dw(residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normality_test(endog,exog):\n",
    "    ''' Checking that the errors are normally distibuted.\n",
    "    The test statistic of the Jarque-Bera test is always a positive number and if itâ€™s far from zero, it indicates that the sample data do not have a normal distribution.\n",
    "    Omnibus P value : A minimum value of 1000 is recommended. Multiple-testing correction provides Bonferroni correction and false discovery rate (FDR). A significance level must be specified in the significance level (a).\n",
    "    '''\n",
    "    model = sm.OLS(endog=endog, exog=exog).fit()\n",
    "    return model.summary().tables[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def homoskedasticity_test(endog,exog):\n",
    "    ''' Often when errors are heteroskedastic they will be greater for greater values of the target. If the target has an exponential distribution, with lots of small values and few large values, then the model will tend to focus on the smaller values in calculating its betas, producing volatility for the higher end of the spectrum. And so we'll see greater divergence in the errors for larger values of the target.\n",
    "    \n",
    "    '''\n",
    "    predictions = sm.OLS(endog=endog, exog=exog).fit().predict(exog)\n",
    "    residuals = endog - predictions\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(endog, residuals)\n",
    "    ax.set_xlabel('Y values')\n",
    "    ax.set_ylabel('Residual Values')\n",
    "    plt.suptitle('Residuals Vs. Predictions');\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
